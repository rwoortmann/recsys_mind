{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch_sparse import SparseTensor, spmm\n",
    "\n",
    "from metrics import *\n",
    "from GNN import *\n",
    "from transformer import SentenceTransformer\n",
    "from graph import *\n",
    "from mind_utils import *\n",
    "\n",
    "TRAIN_S = './data/mind/training_small'\n",
    "TRAIN_L = './data/mind/training_large'\n",
    "VAL_S = '/data/mind/validation_small'\n",
    "VAL_L = '/data/mind/validation_large'\n",
    "TEST = '/data/mind/mind/test_large'\n",
    "\n",
    "log_dir = './runs/tb/'\n",
    "model_dir = './runs/models/'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(exp_name, train_loader, model, epochs, batch_size, test=False):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    exp_dir = log_dir + exp_name\n",
    "    if os.path.exists(exp_dir) and os.path.isdir(exp_dir):\n",
    "        shutil.rmtree(exp_dir) \n",
    "    writer = SummaryWriter(log_dir=exp_dir)\n",
    "\n",
    "    \n",
    "    \n",
    "    losses, pos_accs, neg_accs = [], [], []\n",
    "    for epoch in range(epochs):\n",
    "        for b_i, batch in enumerate(train_loader):\n",
    "\n",
    "            \n",
    "            subgraph, news_x, user_batch, pos_batch, neg_batch = batch\n",
    "            subgraph_feats = model(news_x, subgraph)\n",
    "            \n",
    "            for session in range(batch_size):\n",
    "            \n",
    "                user, pos, neg = user_batch[session], pos_batch[session], neg_batch[session]\n",
    "              \n",
    "                user = torch.tensor([0])\n",
    "                pos = torch.tensor([0, 1])\n",
    "                neg = torch.tensor([3, 4, 5])\n",
    "                n_pos, n_neg = pos.shape[0], neg.shape[0]\n",
    "\n",
    "                user_emb = subgraph_feats['users'][user]\n",
    "                art_emb = subgraph_feats['news'][torch.cat((pos, neg))]\n",
    "\n",
    "                scores = angular_distance(user_emb, art_emb)\n",
    "\n",
    "                targets = torch.zeros(n_pos+n_neg, device=device)\n",
    "                weights = torch.full(targets.shape, 1/n_neg, device=device)\n",
    "                targets[:n_pos] = 1\n",
    "                weights[:n_pos] = 1/n_pos\n",
    "\n",
    "                loss = F.binary_cross_entropy(scores, targets, (weights/2), reduction='sum')\n",
    "\n",
    "                pos_accs.extend((scores[:n_pos].round() == 1).cpu().numpy())\n",
    "                neg_accs.extend((scores[n_pos:].round() == 0).cpu().numpy())\n",
    "                losses.append(loss.item())\n",
    "                \n",
    "    \n",
    "                if (session+1) != batch_size:\n",
    "                    loss.backward(retain_graph=True)\n",
    "                \n",
    "                elif (session+1) == batch_size:\n",
    "                    loss.backward(retain_graph=False)\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    writer.add_scalar('pos_acc', np.mean(pos_accs), (epoch+1)*(b_i * batch_size))\n",
    "                    writer.add_scalar('neg_acc', np.mean(neg_accs), (epoch+1)*(b_i * batch_size))\n",
    "                    writer.add_scalar('loss', np.mean(losses), (epoch+1)*(b_i * batch_size))\n",
    "                    losses, pos_accs, neg_accs = [], [], []\n",
    "    \n",
    "                \n",
    "            \n",
    "            #if (i+1)%5000 == 0:\n",
    "                #writer.close()\n",
    "                #writer = SummaryWriter(log_dir=exp_dir)\n",
    "                \n",
    "\n",
    "            if b_i == 200 and test:\n",
    "                break\n",
    "            \n",
    "        if not test:\n",
    "            torch.save(model.state_dict(), f'{model_dir+exp_name}_epoch_{epoch}')\n",
    "                \n",
    "                \n",
    "    writer.close()\n",
    "    #torch.save(model.state_dict(), model_dir+exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(exp_name, model, writer, iteration):\n",
    "\n",
    "    exp_dir = log_dir + exp_name + '_eval'\n",
    "    if os.path.exists(exp_dir) and os.path.isdir(exp_dir):\n",
    "        shutil.rmtree(exp_dir) \n",
    "    writer = SummaryWriter(log_dir=exp_dir)      \n",
    "    \n",
    "    if not model:\n",
    "        model = GCN(val_data.features['news'].shape[1], empty_nodes, atr_nodes).to(device)\n",
    "        model.load_state_dict(torch.load(model_dir + exp_name), strict=False)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        feats = model(val_data, val_data.edge_index)\n",
    "\n",
    "\n",
    "        aucs, mrrs, ndcg5s, ndcg10s, = [], [], [], []\n",
    "        pos_accs, neg_accs = [], []\n",
    "\n",
    "        for i, batch in enumerate(val_sessions):\n",
    "            user, pos, neg = batch\n",
    "            pos, neg = pos.reshape(-1), neg.reshape(-1)\n",
    "            n_pos, n_neg = pos.shape[0], neg.shape[0]\n",
    "\n",
    "            #subgraph = get_subgraph(val_data, user, pos.reshape(-1), neg.reshape(-1))  \n",
    "            #subgraph_feats = model(val_data, subgraph)\n",
    "            #user_emb = subgraph_feats['users'][user]\n",
    "            #art_emb = torch.index_select(subgraph_feats['news'], 0, torch.cat((pos, neg), dim=1).squeeze())\n",
    "\n",
    "            user_emb = feats['users'][user]\n",
    "            art_emb = feats['news'][torch.cat((pos, neg))]\n",
    "\n",
    "            scores = angular_distance(user_emb, art_emb)\n",
    "\n",
    "            targets = torch.zeros(scores.shape, device=device)\n",
    "            targets[:n_pos] = 1\n",
    "\n",
    "            pos_accs.append((torch.round(scores[:n_pos]) == 1).float().mean().cpu().numpy())\n",
    "            neg_accs.append((torch.round(scores[n_pos:]) == 0).float().mean().cpu().numpy())\n",
    "\n",
    "            #writer.add_scalar('pos_acc', pos_acc, i)\n",
    "            #writer.add_scalar('neg_acc', neg_acc, i)\n",
    "\n",
    "\n",
    "            ranks = (torch.argsort(scores, descending=True)+1).cpu().numpy()\n",
    "            y_score = [1./rank for rank in ranks]\n",
    "            y_true = targets.cpu().numpy()\n",
    "\n",
    "            auc = roc_auc_score(y_true,y_score)\n",
    "            mrr = mrr_score(y_true,y_score)\n",
    "            ndcg5 = ndcg_score(y_true,y_score,5)\n",
    "            ndcg10 = ndcg_score(y_true,y_score,10)\n",
    "\n",
    "            aucs.append(auc)\n",
    "            mrrs.append(mrr)\n",
    "            ndcg5s.append(ndcg5)\n",
    "            ndcg10s.append(ndcg10)\n",
    "                   \n",
    "    \n",
    "    writer.add_scalar('auc', np.mean(aucs), iteration)\n",
    "    writer.add_scalar('mrr', np.mean(mrrs), iteration)\n",
    "    writer.add_scalar('ndcg5', np.mean(ndcg5s), iteration)\n",
    "    writer.add_scalar('ndcg10', np.mean(ndcg10s), iteration)\n",
    "    writer.add_scalar('val_pos_acc', np.mean(pos_accs), iteration)\n",
    "    writer.add_scalar('val_neg_acc', np.mean(neg_accs), iteration)\n",
    "    \n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils.mask import index_to_mask\n",
    "from torch_geometric.utils import dropout_edge\n",
    "\n",
    "class Impressions(Dataset):\n",
    "    def __init__(self, data, set_dir, device=device):\n",
    "        self.device = device\n",
    "        df = get_behaviors(set_dir).dropna(subset=['history'])\n",
    "        \n",
    "        self.user = [data.mapping['users'][user_id] for user_id in df.user_id]\n",
    "        self.pos = [[data.mapping['news'][article] for article in session] for session in df.pos]\n",
    "        self.neg = [[data.mapping['news'][article] for article in session] for session in df.neg]\n",
    "        \n",
    "        self.edge_index = data.edge_index\n",
    "        self.atr_nodes = data.atr_nodes\n",
    "        self.n_nodes = data.n_nodes\n",
    "        self.news_x = data.features['news']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pos)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user = torch.tensor(self.user[idx], dtype=torch.long, device=self.device)\n",
    "        pos = torch.tensor(self.pos[idx], dtype=torch.long, device=self.device)\n",
    "        neg = torch.tensor(self.neg[idx], dtype=torch.long, device=self.device)\n",
    "        return (user, pos, neg)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def bipartite_subgraph(self, subset, edge_index, size, targets):\n",
    "        '''\n",
    "        Relabels the sampled edge_index so that node indices range from [0...n_sampled_nodes] instead of [0...n_all_nodes]\n",
    "        Input:\n",
    "        subset: tuple of (news, atr) nodes to be included\n",
    "        edge_index: edges from sampling\n",
    "        size: amount of nodes in non-sampled graph\n",
    "        '''\n",
    "\n",
    "        src_subset, dst_subset = subset\n",
    "        t_src_subset, t_dst_subset = targets\n",
    "\n",
    "        src_subset = index_to_mask(src_subset, size=size[0])\n",
    "        dst_subset = index_to_mask(dst_subset, size=size[1])\n",
    "        t_src_subset = index_to_mask(t_src_subset, size=size[0])\n",
    "        t_dst_subset = index_to_mask(t_dst_subset, size=size[1])\n",
    "\n",
    "\n",
    "        # relabel nodes\n",
    "        node_idx_i = edge_index.new_zeros(src_subset.size(0))\n",
    "        node_idx_j = edge_index.new_zeros(dst_subset.size(0))\n",
    "\n",
    "        node_idx_i[src_subset] = torch.arange(int(src_subset.sum()),\n",
    "                                              device=node_idx_i.device)\n",
    "\n",
    "        node_idx_j[dst_subset] = torch.arange(int(dst_subset.sum()),\n",
    "                                              device=node_idx_j.device)\n",
    "\n",
    "        edge_index = torch.stack([\n",
    "            node_idx_i[edge_index[0]],\n",
    "            node_idx_j[edge_index[1]],\n",
    "        ], dim=0)\n",
    "\n",
    "        new_articles = node_idx_i[t_src_subset]\n",
    "        new_users = node_idx_j[t_dst_subset]\n",
    "        \n",
    "        return edge_index, new_articles, new_users\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def sampler(self, users, articles):\n",
    "        new_index = {node_type: [] for node_type in self.atr_nodes}\n",
    "        to_sample = {node_type: [] for node_type in self.atr_nodes}\n",
    "\n",
    "        # Hop 1: grab article history of users for which to predict\n",
    "        index = self.edge_index['users']\n",
    "        sample = index[:, torch.isin(index[1], users)]\n",
    "        new_index['users'] = [sample]\n",
    "        to_sample['news'] = torch.cat((sample[0], articles))\n",
    "\n",
    "        # Hop 2: from user history and session postive/negative articles grab connected users/atributes\n",
    "        for atr in self.atr_nodes:\n",
    "            index = self.edge_index[atr]\n",
    "            sample = index[:, torch.isin(index[0], to_sample['news'])]\n",
    "            new_index[atr] += [sample]\n",
    "            to_sample[atr] = sample[1]\n",
    "\n",
    "        # Hop 3: grab articles from these users/atributes\n",
    "        for atr in self.atr_nodes:\n",
    "            index = self.edge_index[atr]\n",
    "            sample = index[:, torch.isin(index[1], to_sample[atr])]\n",
    "            sample, _ = dropout_edge(sample, p=0.7)\n",
    "            new_index[atr] += [sample]\n",
    "\n",
    "        #### create dict\n",
    "        edge_index = {}\n",
    "        for atr, index in new_index.items():\n",
    "            index = torch.cat(index, dim=1).unique(dim=1)\n",
    "            \n",
    "            news_nodes = index[0].unique()\n",
    "            atr_nodes = index[1].unique()\n",
    "            \n",
    "            index, articles, users = self.bipartite_subgraph((news_nodes, atr_nodes), \n",
    "                                               index, \n",
    "                                               (self.n_nodes['news'], self.n_nodes[atr]),\n",
    "                                               (articles, users))\n",
    "            edge_index[atr] = SparseTensor.from_edge_index(index, sparse_sizes=(news_nodes.shape[0], atr_nodes.shape[0]))\n",
    "            \n",
    "            news_x = self.news_x[news_nodes]\n",
    "\n",
    "        return edge_index, news_x, articles, users\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def collate(self, batch):\n",
    "        users, pos, neg = list(zip(*batch))\n",
    "        subgraph, news_x, articles, users = self.sampler(torch.stack(users), torch.cat((*pos, *neg)))\n",
    "        \n",
    "        \n",
    "        return subgraph, news_x, users, pos, neg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys-mind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55587e0a113dde538d0f7e94e64feeec557c7d17d9d5b3f867e14656d8e6f5b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
